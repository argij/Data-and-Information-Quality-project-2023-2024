{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNAoRaZuPuyOI8tjSYfuKuZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pwdq_OjKvbzl"},"outputs":[],"source":["import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import ShuffleSplit, cross_val_score\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import LinearSVR\n","from sklearn.gaussian_process import GaussianProcessRegressor\n","from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n","from sklearn.linear_model import LogisticRegression, BayesianRidge, Ridge\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from sklearn.neural_network import MLPClassifier, MLPRegressor\n","from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, OPTICS, Birch\n","from sklearn import metrics\n","import time as time\n","from sklearn.utils._testing import ignore_warnings\n","from sklearn.exceptions import ConvergenceWarning\n","import pandas as pd\n","\n","#N.B. ONLY FOR PROJECTS ON DATA TYPES: USE THIS FUNCTION TO ENCODE CATEGORICAL VARIABLES BEFORE STANDARDIZATION\n","def encoding_categorical_variables(X):\n","    def encode(original_dataframe, feature_to_encode):\n","        dummies = pd.get_dummies(original_dataframe[[feature_to_encode]], dummy_na=True)\n","        res = pd.concat([original_dataframe, dummies], axis=1)\n","        res = res.drop([feature_to_encode], axis=1)\n","        return (res)\n","\n","    categorical_columns=list(X.select_dtypes(include=['bool','object']).columns)\n","\n","    for col in X.columns:\n","        if col in categorical_columns:\n","            X = encode(X,col)\n","    return X\n","\n","\n","@ignore_warnings(category=ConvergenceWarning)\n","@ignore_warnings(category=FutureWarning)\n","def regression(X, y, regression, seed):\n","    X = StandardScaler().fit_transform(X)\n","    X = np.nan_to_num(X)\n","    regressor = Ridge()\n","\n","    if regression == \"LinearRegressor\":\n","        regressor = Ridge()\n","    elif regression == \"BayesianRidge\":\n","        regressor = BayesianRidge()\n","    elif regression == \"GPRegressor\":\n","        regressor = GaussianProcessRegressor()\n","    elif regression == \"SVMRegressor\":\n","        regressor = LinearSVR()\n","    elif regression == \"KNNRegressor\":\n","        regressor = KNeighborsRegressor()\n","    elif regression == \"MLPRegressor\":\n","        regressor = MLPRegressor()\n","\n","    print(\"Training for \" + regression + \"...\")\n","\n","    start = time.time()\n","\n","    model_fit = regressor.fit(X, y)\n","\n","    cv = ShuffleSplit(n_splits=8, test_size=0.3, random_state=seed)\n","\n","    model_scores = cross_val_score(model_fit, X, y, cv=cv, scoring=\"neg_root_mean_squared_error\")\n","\n","    stop = time.time()\n","    speed = stop - start\n","\n","    mse_mean = abs(model_scores.mean())\n","\n","    return {\"mean_perf\": mse_mean,\n","            \"distance\": distance_measurement(X, y, regression, True, seed),\n","            \"speed\": speed}\n","\n","\n","\n","def distance_measurement(X, y, method, regression, seed):\n","    from sklearn.model_selection import train_test_split\n","    from sklearn.metrics import f1_score, mean_squared_error\n","\n","    X = StandardScaler().fit_transform(X)\n","    X = np.nan_to_num(X)\n","\n","    N = 8\n","    distances_train_test = np.zeros(N)\n","    for i in range(0, N):\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed + i)\n","\n","        model = DecisionTreeClassifier()\n","\n","        if method == \"DecisionTree\":\n","            model = DecisionTreeClassifier()\n","        elif method == \"LogisticRegression\":\n","            model = LogisticRegression()\n","        elif method == \"KNN\":\n","            model = KNeighborsClassifier()\n","        elif method == \"RandomForest\":\n","            model = RandomForestClassifier()\n","        elif method == \"AdaBoost\":\n","            model = AdaBoostClassifier()\n","        elif method == \"MLP\":\n","            model = MLPClassifier()\n","        elif method == \"LinearRegressor\":\n","            model = Ridge()\n","        elif method == \"BayesianRidge\":\n","            model = BayesianRidge()\n","        elif method == \"GPRegressor\":\n","            model = GaussianProcessRegressor()\n","        elif method == \"SVMRegressor\":\n","            model = LinearSVR()\n","        elif method == \"KNNRegressor\":\n","            model = KNeighborsRegressor()\n","        elif method == \"MLPRegressor\":\n","            model = MLPRegressor()\n","\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","        y_fit = model.predict(X_train)\n","\n","        if regression:\n","            mse_pred = abs(mean_squared_error(y_test, y_pred, squared=False))\n","            mse_fit = abs(mean_squared_error(y_train, y_fit, squared=False))\n","\n","            distances_train_test[i] = mse_pred - mse_fit\n","        else:\n","            weighted_f1_pred = f1_score(y_test, y_pred, average='weighted')\n","            weighted_f1_fit = f1_score(y_train, y_fit, average='weighted')\n","\n","            distances_train_test[i] = weighted_f1_fit - weighted_f1_pred\n","\n","    return distances_train_test.mean()"]}]}